# Command one

python starter.py \
    --tinystories_weight 0.0 \
    --input_files 3seqs.txt \
    --block_size 32 \
    --embed_size 64 \
    --prompt "0 1 2"


# Task 1 answers:

â€œFor Task 1, I ran the provided LSTM baseline on a tiny dataset with block_size=32 and confirmed that the model runs end-to-end: batching works, embeddings work, the LSTM outputs logits of shape (seq_len, batch, vocab_size), the loss decreases, and text generation produces tokens. Thatâ€™s the sanity check.â€

â€œWhat is nn.Embedding doing?â€

It maps token IDs to dense vectors, instead of using huge one-hot vectors.
So instead of a 50,000-dimensional one-hot, each token becomes a 64-dim vector.
This makes training efficient and gives the model a learned representation space.

â€œWhy do we do next-token prediction?â€

My model gets a sequence 
ğ‘¥
0
,
ğ‘¥
1
,
â€¦
x
0
	â€‹

,x
1
	â€‹

,â€¦
At position 
ğ‘¡
t, it tries to predict token 
ğ‘¥
ğ‘¡
+
1
x
t+1
	â€‹

That's standard autoregressive training.

â€œHow do you know the code is correct?â€

You show three things:

Loader produces batches of shape (seq_len, batch).

LSTM returns output shape (seq_len, batch, vocab_size).

Loss decreases and text generation prints something coherent.



# Task 2 answers:

Sanity check Command:

python pico-llm.py \
    --tinystories_weight 0.5 \
    --input_files 3seqs.txt \
    --block_size 32 \
    --embed_size 64 \
    --kgram_k 3 \   
    --kgram_chunk_size 1 \
    --num_inner_mlp_layers 1 \
    --max_steps_per_epoch 10 \
    --prompt "0 1 2"

The loss for kgram does down so I can say that it atleast compiles properly.


# Task 3 answers:

python pico-llm.py \
    --tinystories_weight 1 \
    --block_size 256 \
    --embed_size 512 \
    --batch_size 16 \
    --kgram_k 3 \
    --kgram_chunk_size 16 \
    --num_inner_mlp_layers 2 \
    --num_epochs 20 \
    --val_split 10.0 \
    --test_split 20.0 \
    --store_result \
    --prompt "Hi I am a Soldier" \
    --max_new_tokens 50 \
    --model lstm